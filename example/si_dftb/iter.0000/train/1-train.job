#!/bin/sh
#SBATCH --job-name=train1
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:1
#SBATCH --gpus-per-task=1
#SBATCH --partition=new3080ti,3080ti,3090
#SBATCH -x gn43,gn66,login

echo "SLURM_SUBMIT_DIR is $SLURM_SUBMIT_DIR"

echo "Starting job $SLURM_JOB_ID at " `date`

echo "Running on nodes: $SLURM_NODELIST"

start=$(date +%s)
source /data/home/wuxingxing/anaconda3/etc/profile.d/conda.sh
conda activate torch2_feat
export PATH=/data/home/wuxingxing/codespace/PWMLFF_feat/src/bin:$PATH
export PYTHONPATH=/data/home/wuxingxing/codespace/PWMLFF_feat/src/:$PYTHONPATH
module load cuda/11.6
module load intel/2020

{
cd /data/home/wuxingxing/codespace/al_pwmlff/example/si_dftb/iter.0000/temp_run_iter_work/train/train.001
if [ ! -f tag.train.success ] ; then
    PWMLFF train train.json >> train.log

    PWMLFF script ./model_record/dp_model.ckpt >> train.log
    mv ./model_record/torch_script_module.pt .


    if test $? == 0; then
        touch tag.train.success
    else
        touch tag.train.failed
    fi
fi
} &

wait

echo "Job $SLURM_JOB_ID done at " `date`

if [ -f /data/home/wuxingxing/codespace/al_pwmlff/example/si_dftb/iter.0000/temp_run_iter_work/train/train.001/tag.train.success ]; then
    end=$(date +%s)
    take=$(( end - start ))
    echo Time taken to execute commands is ${take} seconds > /data/home/wuxingxing/codespace/al_pwmlff/example/si_dftb/iter.0000/temp_run_iter_work/train/1-tag.train.success
    exit 0
else
    exit 1
fi
